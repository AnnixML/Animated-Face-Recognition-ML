{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-25 14:11:48.216451: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-25 14:11:48.216539: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-25 14:11:48.257398: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-25 14:11:48.351337: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-25 14:11:49.345819: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-02-25 14:11:50.977600: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.130731: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.130825: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.133803: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.133916: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.133990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.435089: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.435218: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.435232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-02-25 14:11:51.435289: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-02-25 14:11:51.435309: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5572 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3486\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../tags_processed_stages/dafre_tags.csv\")\n",
    "num_classes = df['tags_cat4'].nunique()\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463437"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hatsune_miku']                                                                                10644\n",
      "['hakurei_reimu']                                                                                6690\n",
      "['rumia']                                                                                        3803\n",
      "['kochiya_sanae']                                                                                3472\n",
      "['cirno']                                                                                        3470\n",
      "                                                                                                ...  \n",
      "['kirisame_marisa', 'rei_(cookie)']                                                                 1\n",
      "['cecilia_alcott', 'charlotte_dunois', 'huang_lingyin', 'orimura_ichika', 'shinonono_houki']        1\n",
      "['female_gunner_(dungeon_and_fighter)', 'rose_(elsword)']                                           1\n",
      "['oosanshouuo-san', 'ranka_lee']                                                                    1\n",
      "['night_watcher_(elsword)', 'rena_(elsword)']                                                       1\n",
      "Name: tags_cat4, Length: 3486, dtype: int64\n",
      "['hatsune_miku']                           10644\n",
      "['hakurei_reimu']                           6690\n",
      "['rumia']                                   3803\n",
      "['kochiya_sanae']                           3472\n",
      "['cirno']                                   3470\n",
      "                                           ...  \n",
      "['nekomusume']                                18\n",
      "['female_priest_(dungeon_and_fighter)']       16\n",
      "['okonogi_yuuko']                             14\n",
      "[]                                            11\n",
      "['izayoi_(blazblue)']                         10\n",
      "Name: tags_cat4, Length: 3260, dtype: int64\n",
      "3260\n",
      "3260\n",
      "3260\n"
     ]
    }
   ],
   "source": [
    "class_counts = df['tags_cat4'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "def contains_multiple_tags(label, separator=','):\n",
    "    return separator in label\n",
    "filtered_df = df[~df['tags_cat4'].apply(contains_multiple_tags, separator=',')]\n",
    "\n",
    "class_counts = filtered_df['tags_cat4'].value_counts()\n",
    "print(class_counts)\n",
    "\n",
    "train_df, temp_test_df = train_test_split(filtered_df, test_size=0.2, stratify=filtered_df['tags_cat4'])\n",
    "val_df, test_df = train_test_split(temp_test_df, test_size=0.5, stratify=temp_test_df['tags_cat4'])\n",
    "\n",
    "\n",
    "print(train_df['tags_cat4'].nunique())\n",
    "print(val_df['tags_cat4'].nunique())\n",
    "print(test_df['tags_cat4'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 369126 validated image filenames belonging to 3260 classes.\n",
      "Found 46141 validated image filenames belonging to 3260 classes.\n",
      "Found 46141 validated image filenames belonging to 3260 classes.\n",
      "(32, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory='../fullMin256/', \n",
    "    x_col='dir', \n",
    "    y_col='tags_cat4', \n",
    "    target_size=(224, 224), \n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode = 'rgb'\n",
    ")\n",
    "\n",
    "validation_generator = val_datagen.flow_from_dataframe(\n",
    "    dataframe=val_df,\n",
    "    directory='../fullMin256/',\n",
    "    x_col='dir',\n",
    "    y_col='tags_cat4',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=True,\n",
    "    color_mode = 'rgb'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory='../fullMin256/',\n",
    "    x_col='dir',\n",
    "    y_col='tags_cat4',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    color_mode = 'rgb'\n",
    ")\n",
    "x,y  = next(train_generator)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Dropout, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Layer\n",
    "from keras import regularizers\n",
    "from tensorflow import keras\n",
    "#from transformers import BeitImageProcessor, BeitForImageClassification\n",
    "#from keras_cv_attention_models import beit\n",
    "from transformers import AutoImageProcessor, TFViTModel\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.experimental.preprocessing.RandomRotation(0.4),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0.2, width_factor=0),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomTranslation(height_factor=0, width_factor=0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomZoom(0.2, 0.2),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.experimental.preprocessing.RandomContrast(0.2),\n",
    "])\n",
    "class ExtractPoolerOutput(Layer):\n",
    "    def call(self, inputs):\n",
    "        return inputs.pooler_output\n",
    "#base_model = tf.keras.applications.MobileNetV2(input_shape=(96, 96, 3), include_top=False, weights='imagenet')\n",
    "#base_model = TFViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "#image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "base_model = hub.KerasLayer(\"https://www.kaggle.com/models/spsayakpaul/convnext/frameworks/TensorFlow2/variations/base-21k-224/versions/1\", trainable=False)\n",
    "model = Sequential()\n",
    "\n",
    "model.add(data_augmentation)\n",
    "model.add(base_model)\n",
    "#model.add(ExtractPoolerOutput())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(tf.keras.layers.Dense(3260, activation='softmax'))\n",
    "model.build((32, 224, 224, 3))\n",
    "model.summary(show_trainable=True)\n",
    "#model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalFocalCrossentropy(), metrics=['accuracy'])\n",
    "#print(len(base_model.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filer7y_pswq.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filex26rpe4l.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).vit, (), dict(pixel_values=ag__.ld(pixel_values), head_mask=ag__.ld(head_mask), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_filer7y_pswq.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_file_2s7b626.py\", line 24, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileaftycwxb.py\", line 12, in tf__call\n        embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n        projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_vi_t_model_19' (type TFViTModel).\n    \n    in user code:\n    \n        File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 767, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 769, in call  *\n            outputs = self.vit(\n        File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filer7y_pswq.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/tmp/__autograph_generated_file_2s7b626.py\", line 24, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"/tmp/__autograph_generated_fileaftycwxb.py\", line 12, in tf__call\n            embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n            projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 767, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 599, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_fileaftycwxb.py\", line 12, in tf__call\n                embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n            File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n                projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    embeddings = self.patch_embeddings(\n                File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n                    projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n            \n                ValueError: Exception encountered when calling layer 'patch_embeddings' (type TFViTPatchEmbeddings).\n                \n                in user code:\n                \n                    File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 204, in call  *\n                        projection = self.projection(pixel_values)\n                    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n                        raise ValueError(\n                \n                    ValueError: Input 0 of layer \"projection\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (32, 224, 3, 224)\n                \n                \n                Call arguments received by layer 'patch_embeddings' (type TFViTPatchEmbeddings):\n                  • pixel_values=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\n                  • interpolate_pos_encoding=None\n                  • training=False\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\n          • head_mask=tf.Tensor(shape=(32, 3260), dtype=float32)\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_model_19' (type TFViTModel):\n      • pixel_values=('tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(32, 3260), dtype=float32)')\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • training=False\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#model.summary(show_trainable=True)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(), loss\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mCategoricalCrossentropy(label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mTopKCategoricalAccuracy(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_5_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m), tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mTopKCategoricalAccuracy(k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_3_categorical_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)])\n\u001b[0;32m---> 17\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     19\u001b[0m     train_generator,\n\u001b[1;32m     20\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalidation_generator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(validation_generator)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     25\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint, early_stopping, reduce_lr])  \n\u001b[1;32m     27\u001b[0m tra_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:2880\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2876\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\n\u001b[1;32m   2877\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x\n\u001b[1;32m   2878\u001b[0m     )\n\u001b[1;32m   2879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_predict_function()\n\u001b[0;32m-> 2880\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2881\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf_utils\u001b[38;5;241m.\u001b[39msync_to_numpy_or_python_type(outputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filehpmkybxa.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:2425\u001b[0m, in \u001b[0;36mModel.make_predict_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   2421\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2422\u001b[0m     )\n\u001b[1;32m   2424\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 2425\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2426\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   2427\u001b[0m     outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcat\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2428\u001b[0m )\n\u001b[1;32m   2429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:2413\u001b[0m, in \u001b[0;36mModel.make_predict_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 2413\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2414\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `test_step` succeeds.\u001b[39;00m\n\u001b[1;32m   2415\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:2381\u001b[0m, in \u001b[0;36mModel.predict_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2361\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"The logic for one inference step.\u001b[39;00m\n\u001b[1;32m   2362\u001b[0m \n\u001b[1;32m   2363\u001b[0m \u001b[38;5;124;03mThis method can be overridden to support custom inference logic.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2378\u001b[0m \u001b[38;5;124;03m  `Model` on data.\u001b[39;00m\n\u001b[1;32m   2379\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2380\u001b[0m x, _, _ \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39munpack_x_y_sample_weight(data)\n\u001b[0;32m-> 2381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filer7y_pswq.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filex26rpe4l.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, training)\u001b[0m\n\u001b[1;32m      9\u001b[0m do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filer7y_pswq.py:37\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     36\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(func), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m),), \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(unpacked_inputs)), fscope)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file_2s7b626.py:24\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, training)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     23\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mld(pixel_values) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, if_body, else_body, get_state, set_state, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state_1\u001b[39m():\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (head_mask,)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_fileaftycwxb.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[1;32m     10\u001b[0m retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefinedReturnValue()\n\u001b[1;32m     11\u001b[0m (batch_size, num_channels, height, width) \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(shape_list), (ag__\u001b[38;5;241m.\u001b[39mld(pixel_values),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 12\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m cls_tokens \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mrepeat, (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcls_token,), \u001b[38;5;28mdict\u001b[39m(repeats\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(batch_size), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), fscope)\n\u001b[1;32m     14\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mconcat, ((ag__\u001b[38;5;241m.\u001b[39mld(cls_tokens), ag__\u001b[38;5;241m.\u001b[39mld(embeddings)),), \u001b[38;5;28mdict\u001b[39m(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), fscope)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file8c_2k2xs.py:63\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__call\u001b[0;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[1;32m     61\u001b[0m ag__\u001b[38;5;241m.\u001b[39mif_stmt(ag__\u001b[38;5;241m.\u001b[39mnot_(ag__\u001b[38;5;241m.\u001b[39mld(interpolate_pos_encoding)), if_body_3, else_body_3, get_state_3, set_state_3, (), \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     62\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mtranspose, (ag__\u001b[38;5;241m.\u001b[39mld(pixel_values),), \u001b[38;5;28mdict\u001b[39m(perm\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)), fscope)\n\u001b[0;32m---> 63\u001b[0m projection \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m num_patches \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(width) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpatch_size[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m (ag__\u001b[38;5;241m.\u001b[39mld(height) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mpatch_size[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     65\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(tf)\u001b[38;5;241m.\u001b[39mreshape, (), \u001b[38;5;28mdict\u001b[39m(tensor\u001b[38;5;241m=\u001b[39mag__\u001b[38;5;241m.\u001b[39mld(projection), shape\u001b[38;5;241m=\u001b[39m(ag__\u001b[38;5;241m.\u001b[39mld(batch_size), ag__\u001b[38;5;241m.\u001b[39mld(num_patches), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), fscope)\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filer7y_pswq.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_filex26rpe4l.py\", line 11, in tf__call\n        outputs = ag__.converted_call(ag__.ld(self).vit, (), dict(pixel_values=ag__.ld(pixel_values), head_mask=ag__.ld(head_mask), output_attentions=ag__.ld(output_attentions), output_hidden_states=ag__.ld(output_hidden_states), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), return_dict=ag__.ld(return_dict), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_filer7y_pswq.py\", line 37, in tf__run_call_with_unpacked_inputs\n        retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n    File \"/tmp/__autograph_generated_file_2s7b626.py\", line 24, in tf__call\n        embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_fileaftycwxb.py\", line 12, in tf__call\n        embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n    File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n        projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n\n    ValueError: Exception encountered when calling layer 'tf_vi_t_model_19' (type TFViTModel).\n    \n    in user code:\n    \n        File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 767, in run_call_with_unpacked_inputs  *\n            return func(self, **unpacked_inputs)\n        File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 769, in call  *\n            outputs = self.vit(\n        File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n        File \"/tmp/__autograph_generated_filer7y_pswq.py\", line 37, in tf__run_call_with_unpacked_inputs\n            retval_ = ag__.converted_call(ag__.ld(func), (ag__.ld(self),), dict(**ag__.ld(unpacked_inputs)), fscope)\n        File \"/tmp/__autograph_generated_file_2s7b626.py\", line 24, in tf__call\n            embedding_output = ag__.converted_call(ag__.ld(self).embeddings, (), dict(pixel_values=ag__.ld(pixel_values), interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"/tmp/__autograph_generated_fileaftycwxb.py\", line 12, in tf__call\n            embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n        File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n            projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n    \n        ValueError: Exception encountered when calling layer 'vit' (type TFViTMainLayer).\n        \n        in user code:\n        \n            File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/modeling_tf_utils.py\", line 767, in run_call_with_unpacked_inputs  *\n                return func(self, **unpacked_inputs)\n            File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 599, in call  *\n                embedding_output = self.embeddings(\n            File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                raise e.with_traceback(filtered_tb) from None\n            File \"/tmp/__autograph_generated_fileaftycwxb.py\", line 12, in tf__call\n                embeddings = ag__.converted_call(ag__.ld(self).patch_embeddings, (ag__.ld(pixel_values),), dict(interpolate_pos_encoding=ag__.ld(interpolate_pos_encoding), training=ag__.ld(training)), fscope)\n            File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n                projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n        \n            ValueError: Exception encountered when calling layer 'embeddings' (type TFViTEmbeddings).\n            \n            in user code:\n            \n                File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 129, in call  *\n                    embeddings = self.patch_embeddings(\n                File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                    raise e.with_traceback(filtered_tb) from None\n                File \"/tmp/__autograph_generated_file8c_2k2xs.py\", line 63, in tf__call\n                    projection = ag__.converted_call(ag__.ld(self).projection, (ag__.ld(pixel_values),), None, fscope)\n            \n                ValueError: Exception encountered when calling layer 'patch_embeddings' (type TFViTPatchEmbeddings).\n                \n                in user code:\n                \n                    File \"/home/leogu/.local/lib/python3.10/site-packages/transformers/models/vit/modeling_tf_vit.py\", line 204, in call  *\n                        projection = self.projection(pixel_values)\n                    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler  **\n                        raise e.with_traceback(filtered_tb) from None\n                    File \"/home/leogu/.local/lib/python3.10/site-packages/keras/src/engine/input_spec.py\", line 280, in assert_input_compatibility\n                        raise ValueError(\n                \n                    ValueError: Input 0 of layer \"projection\" is incompatible with the layer: expected axis -1 of input shape to have value 3, but received input with shape (32, 224, 3, 224)\n                \n                \n                Call arguments received by layer 'patch_embeddings' (type TFViTPatchEmbeddings):\n                  • pixel_values=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\n                  • interpolate_pos_encoding=None\n                  • training=False\n            \n            \n            Call arguments received by layer 'embeddings' (type TFViTEmbeddings):\n              • pixel_values=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\n              • interpolate_pos_encoding=None\n              • training=False\n        \n        \n        Call arguments received by layer 'vit' (type TFViTMainLayer):\n          • pixel_values=tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)\n          • head_mask=tf.Tensor(shape=(32, 3260), dtype=float32)\n          • output_attentions=False\n          • output_hidden_states=False\n          • interpolate_pos_encoding=None\n          • return_dict=True\n          • training=False\n    \n    \n    Call arguments received by layer 'tf_vi_t_model_19' (type TFViTModel):\n      • pixel_values=('tf.Tensor(shape=(32, 224, 224, 3), dtype=float32)', 'tf.Tensor(shape=(32, 3260), dtype=float32)')\n      • head_mask=None\n      • output_attentions=None\n      • output_hidden_states=None\n      • interpolate_pos_encoding=None\n      • return_dict=None\n      • training=False\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "checkpoint = ModelCheckpoint(\"best_model.keras\", save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, verbose=1, mode='min')\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6 , mode='min', verbose=1)\n",
    "decay_steps = 1000.0\n",
    "initial_learning_rate = 0.0\n",
    "warmup_steps = 1000.0\n",
    "target_learning_rate = 0.1\n",
    "lr_warmup_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate, decay_steps, warmup_target=target_learning_rate,\n",
    "    warmup_steps=warmup_steps\n",
    ")\n",
    "sgd_optimizer = tf.keras.optimizers.SGD(learning_rate=0.05, momentum=0.90)\n",
    "#model.summary(show_trainable=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0), metrics=['accuracy', tf.keras.metrics.TopKCategoricalAccuracy(k=5, name='top_5_categorical_accuracy'), tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_categorical_accuracy')])\n",
    "model.predict_on_batch(next(test_generator))\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=25, \n",
    "    verbose=1,\n",
    "    steps_per_epoch=2000,\n",
    "    validation_steps=len(validation_generator)/32,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr])  \n",
    "\n",
    "tra_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(tra_acc, label = \"Training Accuracy\")\n",
    "plt.plot(val_acc, label = \"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Acc\")\n",
    "\n",
    "tra_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(tra_loss, label = \"Training Loss\")\n",
    "plt.plot(val_loss, label = \"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=100, \n",
    "    verbose=1,\n",
    "    initial_epoch = 0,\n",
    "    steps_per_epoch=500,\n",
    "    validation_steps=len(validation_generator)/32,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr])  \n",
    "\n",
    "tra_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(tra_acc, label = \"Training Accuracy\")\n",
    "plt.plot(val_acc, label = \"Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Acc\")\n",
    "\n",
    "tra_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(tra_loss, label = \"Training Loss\")\n",
    "plt.plot(val_loss, label = \"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss vs Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
